---
title: "R Project - Practical Machine Learning"
author: "Lien Dinh"
date: "11/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# echo = T prints embedded R code, set wd throughout notebook
```

### Initialization & EDA

First, load data and pre-process invalid / empty strings as NA. Data has 160 variables but seems to contain a number of zero / near-zero variables. Testing set has only 20 observations and does not have target variable / label.

```{r results="markup"}
library(tidyverse)
library(caret)
setwd("C:/Users/Administrator/Documents/Learning/R/csv")
set.seed(543671) # set seed
trainingdata <- read.csv("./pml-training.csv",na.strings=c("NA","#DIV/0!"," ", ""))
testingdata <- read.csv("./pml-testing.csv",na.strings=c("NA","#DIV/0!"," ", ""))
```

### Data Cleaning

Create factors / categories out of target variable (stored as str) and perform quick data cleaning to remove NA's along with other irrelevant variables.

```{r results="markup"}
trainingdata$classe <- factor(trainingdata$classe) # convert labels to factors / categories 
training <- trainingdata[,colSums(is.na(testingdata))==0] # select non-NA variables
testing <- testingdata[,colSums(is.na(testingdata))==0]
training <-training[,-c(1:7)] # remove irrelevent columns e.g. X, username
testing <-testing[,-c(1:7)]
nsv <- nearZeroVar(training, saveMetrics = TRUE) # doublecheck that there is no near-zero variables left
nsv # near zero = T means var is likely not very useful as predictor
glimpse(training) # glimpse that there's no zero variables, irrelevant variables in the 53 that remain
```

### Create Validation Set

Given that the test set has no labels and training set has a large number of observations (19622), we can afford to partition out 70% into a in-training test set for cross validation.
r
```{r results="markup"}
intrain <- createDataPartition(y=training$classe,p=.7,list=FALSE)
train <- training[intrain,] # select all intrain rows
traincv <- training[-intrain,]
dim(train) ; dim(traincv) # check that 53 variables remain, ratio is 70-30 between train/traincv
```

### Fitting Models

Target variable is categorical (5 factors), hence linear models likely will not perform well. We can attempt to fit (1) a simple classification tree, (2) bagging, (3) random forest with 10-fold cross validation, and (4) boosting. 

1. Classification Tree

```{r results="markup"}
library(rpart)
library(rpart.plot)
model_tree <- train(classe~.,data=train,method="rpart")
model_tree$results # see cp, accuracy and kappa by the splits
# Apply, Visualize & Assess Model
predictions_tree <- predict(model_tree,traincv)
rpart.plot(model_tree$finalModel) # plot tree: some overfitting clearly present
matrix_tree <- confusionMatrix(predictions_tree,traincv$classe)
matrix_tree # low accuracy, close to random
```

2. Bagging / Bootstrap Aggregating

```{r results="markup"}
model_bg <- train(classe~.,data=train,method="treebag") # bagging method inbuilt in R
predictions_bg <- predict(model_bg,traincv) # make predictions
matrix_bg <- confusionMatrix(predictions_bg,traincv$classe) # assess
matrix_bg # much better accuracy than a simple tree ~ 98.5% 
```

3. Random Forest with 10-fold cross-validation

```{r results="markup"}
controls <- trainControl(method = "repeatedcv", # cross validation
                         number = 10, # 10-fold
                         repeats = 2) # repeated twice
model_rf <- train(classe~., 
                data = train,
                trControl = controls,
                method = "rf") 
predictions_rf <- predict(model_rf,traincv) # make predictions
matrix_rf <- confusionMatrix(predictions_rf,traincv$classe) # assess
matrix_rf # good accuracy ~ 99.2%, although runtime is extremely slow
```

4. Boosting with preprocessing

```{r results="markup"}
model_boost <- train(classe~., 
                data = train,
                preProcess = c("center","scale"), # center and scale all variables
                method = "gbm",
                verbose=FALSE) 
predictions_boost <- predict(model_boost,traincv) # make predictions
matrix_boost <- confusionMatrix(predictions_boost,traincv$classe) # assess
matrix_boost # good accuracy as well
```


### Model Comparison & Selection

Compile performance statistics on the 4 generated models to compare. Random forest model has the best overall accuracy.

```{r results="markup"}
df_matrices <- data.frame(matrix_tree$overall, matrix_bg$overall, matrix_rf$overall, matrix_boost$overall)
df_matrices
```

### Final Predictions

Use random forest model to generate final predictions on the test set.

```{r results="markup"}
### FINAL PREDICTIONS
predictions_final <- predict(model_rf, testing)
predictions_final
```


